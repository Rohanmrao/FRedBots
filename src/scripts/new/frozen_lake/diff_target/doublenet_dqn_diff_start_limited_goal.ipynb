{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q Network for navigating through a grid world with different (LIMITED) goal locations\n",
    "#### Reward system 1: Negative of euclidean distance to goal - Did not converge for 250 episodes\n",
    "#### Reward system 2: As per the paper - \"Autonomous Navigation of an AMR using Deep Reinforcement Learning in a Warehouse Environment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print gpu info\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a neural network model\n",
    "def create_model(input_shape, num_actions):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(num_actions, activation='linear')\n",
    "    ])\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class for the environment\n",
    "class Env():\n",
    "    def __init__(self, grid_size=6, max_steps=500):\n",
    "        self.grid_size = grid_size\n",
    "        self.max_steps = max_steps\n",
    "        self.goal_lst = [np.array([2, 3]), np.array([4, 3]), np.array([0, 4])]\n",
    "        # self.goal = np.random.randint(0, grid_size, size=2) # random goal\n",
    "        self.goal = random.choice(self.goal_lst)\n",
    "        print('Goal:', self.goal)\n",
    "\n",
    "        self.rewards = np.zeros((grid_size, grid_size))\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.pos = np.random.randint(0, self.grid_size, size=2)\n",
    "        self.steps = 0\n",
    "        self.done = False\n",
    "        return self.pos\n",
    "    \n",
    "    def reset_goal(self):\n",
    "        # self.goal = np.random.randint(0, self.grid_size, size=2) # random goal\n",
    "        self.goal = random.choice(self.goal_lst)\n",
    "        print('Goal:', self.goal)\n",
    "        # for i in range(self.grid_size):\n",
    "        #     for j in range(self.grid_size):\n",
    "        #         self.rewards[i, j] = -self.euclidean_distance_from_goal(np.array([i, j]))\n",
    "        # self.rewards[self.goal[0], self.goal[1]] = 100\n",
    "        return self.goal\n",
    "    \n",
    "    # def step(self, action):\n",
    "    #     self.steps += 1\n",
    "    #     if action == 0 and self.pos[0] < self.grid_size - 1: # right\n",
    "    #         self.pos[0] += 1\n",
    "    #     elif action == 1 and self.pos[0] > 0: # left\n",
    "    #         self.pos[0] -= 1\n",
    "    #     elif action == 2 and self.pos[1] > 0: # down\n",
    "    #         self.pos[1] -= 1\n",
    "    #     elif action == 3 and self.pos[1] < self.grid_size - 1: # up\n",
    "    #         self.pos[1] += 1\n",
    "    #     else:\n",
    "    #         pass\n",
    "    #         # raise ValueError('Invalid action')\n",
    "    #     if np.array_equal(self.pos, self.goal):\n",
    "    #         self.done = True\n",
    "    #         reward = 0\n",
    "    #         # reward = 100\n",
    "    #     elif self.steps >= self.max_steps:\n",
    "    #         self.done = True\n",
    "    #         reward = self.rewards[self.pos[0], self.pos[1]]\n",
    "    #     else:\n",
    "    #         reward = self.rewards[self.pos[0], self.pos[1]]\n",
    "    #     return self.pos, reward, self.done\n",
    "\n",
    "    def step(self, action): # As per the paper\n",
    "        self.steps += 1\n",
    "        prev_pos = self.pos.copy()\n",
    "        if action == 0 and self.pos[0] < self.grid_size - 1: # right\n",
    "            self.pos[0] += 1\n",
    "        elif action == 1 and self.pos[0] > 0: # left\n",
    "            self.pos[0] -= 1\n",
    "        elif action == 2 and self.pos[1] > 0: # down\n",
    "            self.pos[1] -= 1\n",
    "        elif action == 3 and self.pos[1] < self.grid_size - 1: # up\n",
    "            self.pos[1] += 1\n",
    "        else:\n",
    "            reward = -150\n",
    "            self.done = True\n",
    "            return self.pos, reward, self.done # TODO: The episode is not terminated.\n",
    "        if np.array_equal(self.pos, self.goal):\n",
    "            self.done = True\n",
    "            reward = 500\n",
    "        elif self.steps >= self.max_steps:\n",
    "            self.done = True\n",
    "            reward = self.rewards[self.pos[0], self.pos[1]]\n",
    "        else:\n",
    "            if self.euclidean_distance_from_goal(self.pos) < self.euclidean_distance_from_goal(prev_pos):\n",
    "                reward = 10\n",
    "            else:\n",
    "                reward = -10\n",
    "        return self.pos, reward, self.done\n",
    "        \n",
    "\n",
    "    def euclidean_distance_from_goal(self, pos):\n",
    "        dist = np.sqrt(np.sum((pos - self.goal) ** 2))\n",
    "        return dist\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent class\n",
    "class Agent():\n",
    "    def __init__(self, env, model, target_model):\n",
    "        self.env = env\n",
    "        self.model = model\n",
    "        self.target_model = target_model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        self.gamma = 0.7\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.01\n",
    "        self.batch_size = 64\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        \n",
    "\n",
    "    def add_to_memory(self, state, goal, action, reward, next_state, done):\n",
    "        self.memory.append((state, goal, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(0, 4)\n",
    "        else:\n",
    "            # Pre-process the input\n",
    "            inputt = np.concatenate((state, self.env.goal))\n",
    "            inputt = tf.convert_to_tensor(inputt)\n",
    "            inputt = tf.expand_dims(inputt, 0)\n",
    "\n",
    "            return np.argmax(self.model.predict(inputt, verbose=0)[0]) # TODO: check the predict output\n",
    "\n",
    "    def predict(self, inputt):\n",
    "\n",
    "        return np.argmax(self.model.predict(inputt, verbose=0)[0])\n",
    "\n",
    "    def replay(self):\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        for state, goal, action, reward, next_state, done in batch:\n",
    "            \n",
    "            target = reward\n",
    "\n",
    "            if not done:\n",
    "                # Pre-process the next state input\n",
    "                input_next = np.concatenate((next_state, goal))\n",
    "                input_next = tf.convert_to_tensor(input_next)\n",
    "                input_next = tf.expand_dims(input_next, 0)\n",
    "\n",
    "                target += self.gamma * np.amax(self.target_model.predict(input_next, verbose=0)[0])\n",
    "\n",
    "            # Pre-process the current state input\n",
    "            inputt = np.concatenate((state, goal))\n",
    "            inputt = tf.convert_to_tensor(inputt)\n",
    "            inputt = tf.expand_dims(inputt, 0)\n",
    "\n",
    "            cur_q_value = self.model.predict(inputt, verbose=0) # Q-value of current state\n",
    "            cur_q_value[0][action] = target # TODO: check the predict output\n",
    "            \n",
    "            self.model.fit(inputt, cur_q_value, epochs=1, verbose=0)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agent\n",
    "\n",
    "try:\n",
    "    model = tf.keras.models.load_model('doublenet_dqn_diff_start_diff_goal_limited.h5')\n",
    "    print(\"Loaded model from disk\")\n",
    "    agent = Agent(Env(), model=model, target_model=model)\n",
    "except:\n",
    "    print(\"Creating new model\")\n",
    "    agent = Agent(Env(), model=create_model(input_shape=(4,), num_actions=4), target_model=create_model(input_shape=(4,), num_actions=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "\n",
    "num_episodes = 150\n",
    "# num_episodes = 5\n",
    "reward_lst = []\n",
    "\n",
    "file = open('rewards.txt', 'a')\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state_lst = [] # DEBUG\n",
    "    ep_reward = 0 # DEBUG\n",
    "    state = agent.env.reset()\n",
    "    for step in range(agent.env.max_steps):\n",
    "        state_lst.append(state.copy()) # DEBUG\n",
    "        # print('State:', state) # DEBUG\n",
    "        # print('state_lst:', state_lst) # DEBUG\n",
    "        action = agent.act(state)\n",
    "        # print('Action:', action) # DEBUG\n",
    "        next_state, reward, done = agent.env.step(action)\n",
    "        ep_reward += reward # DEBUG\n",
    "        # print(f\"next_state: {next_state}, reward: {reward}, done: {done}\") # DEBUG\n",
    "        # next_state = np.reshape(next_state, [1, 2])\n",
    "        agent.add_to_memory(state, agent.env.goal, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            if np.array_equal(agent.env.goal, agent.env.pos): # Reached the goal\n",
    "                agent.env.reset_goal()\n",
    "            if agent.env.steps >= agent.env.max_steps:\n",
    "                print('Episode: {}/{}, steps: {}, e: {:.2}'.format(episode, num_episodes, step+1, agent.epsilon))\n",
    "                print('State list:', state_lst) # DEBUG\n",
    "                break\n",
    "    if len(agent.memory) > agent.batch_size:\n",
    "        agent.replay()\n",
    "    if agent.epsilon > agent.epsilon_min:\n",
    "        agent.epsilon = (1 - agent.epsilon_min) * np.exp(-agent.epsilon_decay*episode) + agent.epsilon_min\n",
    "    if episode % 5 == 0:\n",
    "        agent.model.save('doublenet_dqn_diff_start_diff_goal_limited.h5')    \n",
    "    agent.target_model.set_weights(agent.model.get_weights())\n",
    "\n",
    "    reward_lst.append(ep_reward) # DEBUG\n",
    "    file.write(f\"{episode},{ep_reward}\\n\") # DEBUG\n",
    "    file.flush() # DEBUG\n",
    "    \n",
    "print('Average reward:', np.mean(reward_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent\n",
    "\n",
    "state = agent.env.reset()\n",
    "goal = agent.env.reset_goal()\n",
    "print('Goal:', goal)\n",
    "for step in range(agent.env.max_steps):\n",
    "    print('State:', state)\n",
    "    inputt = np.concatenate((state, goal))\n",
    "    inputt = tf.convert_to_tensor(inputt)\n",
    "    inputt = tf.expand_dims(inputt, 0)\n",
    "    action = agent.predict(inputt)\n",
    "    next_state, reward, done = agent.env.step(action)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        print('Steps: ', step+1)\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(reward_lst)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "# PLot mean line for 10 episodes\n",
    "mean_lst = []\n",
    "for i in range(len(reward_lst)):\n",
    "    if i < 10:\n",
    "        mean_lst.append(np.mean(reward_lst[:i+1]))\n",
    "    else:\n",
    "        mean_lst.append(np.mean(reward_lst[i-10:i+1]))\n",
    "plt.plot(mean_lst)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
