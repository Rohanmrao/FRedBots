{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Q Network for navigating through a grid world\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print gpu info\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a neural network model\n",
    "def create_model(input_shape, num_actions):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(num_actions, activation='linear')\n",
    "    ])\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.01))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class for the environment\n",
    "class Env():\n",
    "    def __init__(self, grid_size=6, max_steps=50):\n",
    "        self.grid_size = grid_size\n",
    "        self.max_steps = max_steps\n",
    "        # self.goal = np.random.randint(0, grid_size, size=2) # random goal\n",
    "        self.goal = np.array([4, 3]) # fixed goal\n",
    "        print('Goal:', self.goal)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.pos = np.random.randint(0, self.grid_size, size=2)\n",
    "        self.steps = 0\n",
    "        self.done = False\n",
    "        return self.pos\n",
    "    \n",
    "    def step(self, action):\n",
    "        prev_pos = self.pos.copy()\n",
    "        self.steps += 1\n",
    "        if action == 0: # up\n",
    "            self.pos[0] = min(self.pos[0] + 1, self.grid_size - 1)\n",
    "        elif action == 1: # down\n",
    "            self.pos[0] = max(self.pos[0] - 1, 0)\n",
    "        elif action == 2: # left\n",
    "            self.pos[1] = max(self.pos[1] - 1, 0)\n",
    "        elif action == 3: # right\n",
    "            self.pos[1] = min(self.pos[1] + 1, self.grid_size - 1)\n",
    "        else:\n",
    "            raise ValueError('Invalid action')\n",
    "        if np.array_equal(self.pos, self.goal):\n",
    "            self.done = True\n",
    "            reward = 100\n",
    "        elif self.steps >= self.max_steps:\n",
    "            self.done = True\n",
    "            reward = 0\n",
    "        else:\n",
    "            if self.euclidean_distance_from_goal(self.pos) < self.euclidean_distance_from_goal(prev_pos):\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = -1\n",
    "        return self.pos, reward, self.done\n",
    "\n",
    "    def euclidean_distance_from_goal(self, pos):\n",
    "        dist = np.sqrt(np.sum((pos - self.goal) ** 2))\n",
    "        return dist\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent class\n",
    "class Agent():\n",
    "    def __init__(self, env, model):\n",
    "        self.env = env\n",
    "        self.model = model\n",
    "        self.gamma = 0.7\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.05\n",
    "        self.batch_size = 32\n",
    "        self.memory = deque(maxlen=1000)\n",
    "        self.target_model = create_model((1, 2), 4)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def add_to_memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(0, 4)\n",
    "        else:\n",
    "            # Pre-process the state\n",
    "            state = tf.convert_to_tensor(state)\n",
    "            state = tf.expand_dims(state, 0)\n",
    "\n",
    "            return np.argmax(self.model.predict(state, verbose=0)[0][0]) # TODO: check the predict output\n",
    "\n",
    "    def predict(self, state):\n",
    "        # Pre-process the state\n",
    "        state = tf.convert_to_tensor(state)\n",
    "        state = tf.expand_dims(state, 0)\n",
    "\n",
    "        return np.argmax(self.model.predict(state, verbose=0)[0][0])\n",
    "\n",
    "    def replay(self):\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            \n",
    "            target = reward\n",
    "\n",
    "            if not done:\n",
    "                # Pre-process the next state\n",
    "                next_state = tf.convert_to_tensor(next_state)\n",
    "                next_state = tf.expand_dims(next_state, 0)\n",
    "\n",
    "                target += self.gamma * np.amax(self.target_model.predict(next_state, verbose=0)[0])\n",
    "\n",
    "            # Pre-process the state\n",
    "            state = tf.convert_to_tensor(state)\n",
    "            state = tf.expand_dims(state, 0)\n",
    "\n",
    "            cur_q_value = self.model.predict(state, verbose=0) # Q-value of current state\n",
    "            cur_q_value[0][0][action] = target\n",
    "            \n",
    "            self.model.fit(state, cur_q_value, epochs=1, verbose=0)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agent\n",
    "\n",
    "try:\n",
    "    model = tf.keras.models.load_model('frozen_lake_dqn_diff_start.h5')\n",
    "    print(\"Loaded model from disk\")\n",
    "    agent = Agent(Env(), model=model)\n",
    "except:\n",
    "    print(\"Creating new model\")\n",
    "    agent = Agent(Env(), model=create_model(input_shape=(1, 2), num_actions=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "\n",
    "num_episodes = 50\n",
    "for episode in range(num_episodes):\n",
    "    state_lst = [] # DEBUG\n",
    "    state = agent.env.reset()\n",
    "    state = np.reshape(state, [1, 2])\n",
    "    for step in range(agent.env.max_steps):\n",
    "        state_lst.append(state.copy()) # DEBUG\n",
    "        # print('State:', state) # DEBUG\n",
    "        # print('state_lst:', state_lst) # DEBUG\n",
    "        action = agent.act(state)\n",
    "        # print('Action:', action) # DEBUG\n",
    "        next_state, reward, done = agent.env.step(action)\n",
    "        # print(f\"next_state: {next_state}, reward: {reward}, done: {done}\") # DEBUG\n",
    "        next_state = np.reshape(next_state, [1, 2])\n",
    "        agent.add_to_memory(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print('Episode: {}/{}, score: {}, e: {:.2}'.format(episode, num_episodes, step, agent.epsilon))\n",
    "            print('State list:', state_lst) # DEBUG\n",
    "            break\n",
    "        if len(agent.memory) > agent.batch_size:\n",
    "            agent.replay()\n",
    "        if agent.epsilon > agent.epsilon_min:\n",
    "            agent.epsilon = (1 - agent.epsilon_min) * np.exp(-agent.epsilon_decay*episode) + agent.epsilon_min\n",
    "        \n",
    "    agent.target_model.set_weights(agent.model.get_weights())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent\n",
    "\n",
    "state = agent.env.reset()\n",
    "state = np.reshape(state, [1, 2])\n",
    "for step in range(agent.env.max_steps):\n",
    "    print('State:', state)\n",
    "    action = agent.predict(state)\n",
    "    next_state, reward, done = agent.env.step(action)\n",
    "    next_state = np.reshape(next_state, [1, 2])\n",
    "    state = next_state\n",
    "    if done:\n",
    "        print('Score:', step)\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.model.save('doublenet_dqn_diff_start.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
