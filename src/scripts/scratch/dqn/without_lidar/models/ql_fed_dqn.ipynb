{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_col = 21\n",
    "env_row = 21\n",
    "\n",
    "epsilon = 0.8\n",
    "discount_factor = 0.9\n",
    "learning_rate = 0.1\n",
    "\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "obstacles = [[3, 0], [3, 1], [3, 2], [3, 3], [3, 4], [3, 5], [3, 6], [3, 7], [3, 8], [3, 9],\n",
    "             [8, 9], [8, 10], [8, 11], [8, 12], [8, 13], [8, 14], [8, 15], [8, 16], [8, 17], [8, 18], [8, 19], [8, 20],\n",
    "             [14, 0], [14, 1], [14, 2], [14, 3], [14, 4], [14, 5], [14, 6], [14, 7], [14, 8], [14, 9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.full((env_row, env_col), -1)\n",
    "for x,y in obstacles:\n",
    "    rewards[x,y] = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a neural network model\n",
    "def create_model(input_shape, num_actions):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(num_actions, activation='linear')\n",
    "    ])\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_starting_location():\n",
    "    # get a random row and column index\n",
    "    current_row_index = np.random.randint(env_row)\n",
    "    current_column_index = np.random.randint(env_col)\n",
    "    # if the random location is an obstacle, keep getting a new one\n",
    "    while [current_row_index, current_column_index] in obstacles:\n",
    "        current_row_index = np.random.randint(env_row)\n",
    "        current_column_index = np.random.randint(env_col)\n",
    "    return current_row_index, current_column_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_final_state(row, col, goal_x, goal_y):\n",
    "    if row == goal_x and col == goal_y:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_action(q_values, current_row_index, current_column_index, epsilon):\n",
    "    # if a randomly chosen value between 0 and 1 is less than epsilon,\n",
    "    # then choose the most promising value from the Q-table for this state.\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.argmax(q_values[current_row_index, current_column_index])\n",
    "    else:  # choose a random action\n",
    "        return np.random.randint(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_location(current_row_index, current_column_index, action_index):\n",
    "    new_row_index = current_row_index\n",
    "    new_column_index = current_column_index\n",
    "    if actions[action_index] == 'up' and current_row_index < env_row - 1:\n",
    "        new_row_index += 1\n",
    "    elif actions[action_index] == 'down' and current_row_index > 0:\n",
    "        new_row_index -= 1\n",
    "    elif actions[action_index] == 'right' and current_column_index < env_col - 1:\n",
    "        new_column_index += 1\n",
    "    elif actions[action_index] == 'left' and current_column_index > 0:\n",
    "        new_column_index -= 1\n",
    "\n",
    "    return new_row_index, new_column_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_q_table(target):\n",
    "\n",
    "    rewards[target[0], target[1]] = 100\n",
    "\n",
    "    q_values = np.zeros((env_row, env_col, len(actions)))\n",
    "    \n",
    "    for episode in range(1000):\n",
    "        # get the starting location for this episode\n",
    "        row_index, column_index = get_starting_location()\n",
    "        # continue taking actions (i.e., moving) until we reach a terminal state\n",
    "        # (i.e., until we reach the item packaging area or crash into an item storage location)\n",
    "        while not is_final_state(row_index, column_index, target[0], target[1]):\n",
    "            # choose which action to take (i.e., where to move next)\n",
    "            action_index = get_next_action(q_values, row_index, column_index, epsilon)\n",
    "            # perform the chosen action, and transition to the next state (i.e., move to the next location)\n",
    "            # store the old row and column indexes\n",
    "            old_row_index, old_column_index = row_index, column_index\n",
    "            row_index, column_index = get_next_location(row_index, column_index, action_index)\n",
    "            # receive the reward for moving to the new state, and calculate the temporal difference\n",
    "            reward = rewards[row_index, column_index]\n",
    "            old_q_value = q_values[old_row_index,\n",
    "                                   old_column_index, action_index]\n",
    "            temporal_difference = reward + (discount_factor *np.max(q_values[row_index, column_index])) - old_q_value\n",
    "            # update the Q-value for the previous state and action pair\n",
    "            new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "            q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
    "\n",
    "    rewards[target[0], target[1]] = -1\n",
    "\n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    q_tables = pickle.load(open(\"q_tables.bin\", \"rb\"))\n",
    "except: # Takes about 1 minute to generate q_tables for 21x21 grid\n",
    "    print(\"No q_tables.bin found, initializing new q_tables\")\n",
    "    q_tables = {}\n",
    "    for i in range(env_row):\n",
    "        for j in range(env_col):\n",
    "            if [i, j] not in obstacles:\n",
    "                q_tables[(i, j)] = generate_q_table([i, j])\n",
    "\n",
    "    filee = open(\"q_tables.bin\", \"wb\")\n",
    "    pickle.dump(q_tables, filee)\n",
    "    filee.close()\n",
    "    print(\"Q tables initialized and saved to q_tables.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model = tf.keras.models.load_model('models/ql_fed_dqn.h5')\n",
    "    print('Loaded model from disk')\n",
    "except:\n",
    "    print('No model found, creating new one')\n",
    "    model = create_model((4,), len(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_memory(batch_size):\n",
    "    memory = []\n",
    "    for i in range(batch_size):\n",
    "        state = get_starting_location()\n",
    "        target = get_starting_location()\n",
    "        while target == state:\n",
    "            target = get_starting_location()\n",
    "        \n",
    "        q_values = q_tables[target][state[0]][state[1]]\n",
    "\n",
    "        memory.append((state, target, q_values))\n",
    "    return memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batch_size, epoch=1):\n",
    "    \n",
    "    # Generate a random batch of experiences\n",
    "    mem = generate_memory(batch_size)\n",
    "    state = np.array([i[0] for i in mem])\n",
    "    target = np.array([i[1] for i in mem])\n",
    "    q_values = np.array([i[2] for i in mem])\n",
    "\n",
    "    # Modify the input to contain both the state and the target\n",
    "    inputt = np.concatenate((state, target), axis=1)\n",
    "\n",
    "    # Train the model using the experiences\n",
    "    model.fit(inputt, q_values, epochs=epoch, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_size):\n",
    "\n",
    "    # Generate a random batch of experiences\n",
    "    mem = generate_memory(test_size)\n",
    "    state = np.array([i[0] for i in mem])\n",
    "    target = np.array([i[1] for i in mem])\n",
    "    q_values = np.array([i[2] for i in mem])\n",
    "\n",
    "    # Modify the input to contain both the state and the target\n",
    "    inputt = np.concatenate((state, target), axis=1)\n",
    "\n",
    "    correct_action = 0\n",
    "\n",
    "    for i in range(test_size):\n",
    "        # Get the predicted q values\n",
    "        q_values_pred = model.predict(inputt[i].reshape(1, 4), verbose=0)\n",
    "\n",
    "        # Check if the predicted q values are equal to the actual q values\n",
    "        if np.argmax(q_values_pred) == np.argmax(q_values[i]):\n",
    "            correct_action += 1\n",
    "    \n",
    "    # Return the accuracy\n",
    "    return correct_action / test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/500: 0.42\n",
      "25/500: 0.5\n",
      "50/500: 0.32\n",
      "75/500: 0.44\n",
      "100/500: 0.44\n",
      "125/500: 0.54\n",
      "150/500: 0.52\n",
      "175/500: 0.38\n",
      "200/500: 0.48\n",
      "225/500: 0.34\n",
      "250/500: 0.36\n",
      "275/500: 0.42\n",
      "300/500: 0.6\n",
      "325/500: 0.48\n",
      "350/500: 0.42\n",
      "375/500: 0.48\n",
      "400/500: 0.4\n",
      "425/500: 0.5\n",
      "450/500: 0.5\n",
      "475/500: 0.4\n"
     ]
    }
   ],
   "source": [
    "max_episodes = 500\n",
    "for i in range(max_episodes):\n",
    "    train(10000, 25)\n",
    "    if i % 25 == 0:\n",
    "        print(f\"{i}/{max_episodes}: {test(50)}\")\n",
    "        model.save(\"models/ql_fed_dqn.h5\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.464"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
