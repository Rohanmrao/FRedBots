{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class for the environment\n",
    "class Env():\n",
    "    def __init__(self, grid_size=21, max_steps=500):\n",
    "        self.grid_size = grid_size\n",
    "        self.max_steps = max_steps\n",
    "        \n",
    "        self.obstacles = np.array([[3, 0], [3, 1], [3, 2], [3, 3], [3, 4], [3, 5], [3, 6], [3, 7], [3, 8], [3, 9],\n",
    "             [8, 9], [8, 10], [8, 11], [8, 12], [8, 13], [8, 14], [8, 15], [8, 16], [8, 17], [8, 18], [8, 19], [8, 20],\n",
    "             [14, 0], [14, 1], [14, 2], [14, 3], [14, 4], [14, 5], [14, 6], [14, 7], [14, 8], [14, 9]])\n",
    "        # self.rewards = np.zeros((grid_size, grid_size))\n",
    "        self.reset()\n",
    "        self.reset_goal()\n",
    "\n",
    "    def check(self, test, array):\n",
    "        return any(np.array_equal(x, test) for x in array)\n",
    "\n",
    "    def reset(self):\n",
    "        self.pos = np.random.randint(0, self.grid_size, size=2)\n",
    "        while self.check(self.pos, self.obstacles):\n",
    "            self.pos = np.random.randint(0, self.grid_size, size=2)\n",
    "        self.steps = 0\n",
    "        self.done = False\n",
    "        return self.pos\n",
    "    \n",
    "    def reset_goal(self):\n",
    "        self.goal = np.random.randint(0, self.grid_size, size=2)\n",
    "        while self.check(self.goal, self.obstacles) or np.array_equal(self.pos, self.goal):\n",
    "            self.goal = np.random.randint(0, self.grid_size, size=2)\n",
    "        print('Goal:', self.goal)\n",
    "        # for i in range(self.grid_size):\n",
    "        #     for j in range(self.grid_size):\n",
    "        #         self.rewards[i, j] = -self.euclidean_distance_from_goal(np.array([i, j]))\n",
    "        # self.rewards[self.goal[0], self.goal[1]] = 100\n",
    "        return self.goal\n",
    "    \n",
    "    # def step(self, action):\n",
    "    #     self.steps += 1\n",
    "    #     if action == 0 and self.pos[0] < self.grid_size - 1: # right\n",
    "    #         self.pos[0] += 1\n",
    "    #     elif action == 1 and self.pos[0] > 0: # left\n",
    "    #         self.pos[0] -= 1\n",
    "    #     elif action == 2 and self.pos[1] > 0: # down\n",
    "    #         self.pos[1] -= 1\n",
    "    #     elif action == 3 and self.pos[1] < self.grid_size - 1: # up\n",
    "    #         self.pos[1] += 1\n",
    "    #     else:\n",
    "    #         pass\n",
    "    #         # raise ValueError('Invalid action')\n",
    "    #     if np.array_equal(self.pos, self.goal):\n",
    "    #         self.done = True\n",
    "    #         reward = 0\n",
    "    #         # reward = 100\n",
    "    #     elif self.steps >= self.max_steps:\n",
    "    #         self.done = True\n",
    "    #         reward = self.rewards[self.pos[0], self.pos[1]]\n",
    "    #     else:\n",
    "    #         reward = self.rewards[self.pos[0], self.pos[1]]\n",
    "    #     return self.pos, reward, self.done\n",
    "\n",
    "    def step(self, action): # As per the paper\n",
    "        self.steps += 1\n",
    "        prev_pos = self.pos.copy()\n",
    "        if action == 0 and self.pos[0] < self.grid_size - 1 and not (self.check(self.pos + np.array([1, 0]), self.obstacles)): # right\n",
    "            self.pos[0] += 1\n",
    "        elif action == 1 and self.pos[0] > 0 and not (self.check(self.pos - np.array([1, 0]), self.obstacles)): # left\n",
    "            self.pos[0] -= 1\n",
    "        elif action == 2 and self.pos[1] > 0 and not (self.check(self.pos - np.array([0, 1]), self.obstacles)): # down\n",
    "            self.pos[1] -= 1\n",
    "        elif action == 3 and self.pos[1] < self.grid_size - 1 and not (self.check(self.pos + np.array([0, 1]), self.obstacles)): # up\n",
    "            self.pos[1] += 1\n",
    "        else:\n",
    "            reward = -150\n",
    "            self.done = False\n",
    "            return self.pos, reward, self.done, True # TODO: The episode is not terminated.\n",
    "        if np.array_equal(self.pos, self.goal):\n",
    "            self.done = True\n",
    "            reward = 500\n",
    "        elif self.steps >= self.max_steps:\n",
    "            self.done = True\n",
    "            reward = 0\n",
    "        else:\n",
    "            if self.euclidean_distance_from_goal(self.pos) < self.euclidean_distance_from_goal(prev_pos):\n",
    "                reward = 10\n",
    "            else:\n",
    "                reward = -10\n",
    "        return self.pos, reward, self.done, False\n",
    "        \n",
    "\n",
    "    def euclidean_distance_from_goal(self, pos):\n",
    "        dist = np.sqrt(np.sum((pos - self.goal) ** 2))\n",
    "        return dist\n",
    "\n",
    "    def get_orientation(self, prev_state, curr_state):\n",
    "        if curr_state[0] > prev_state[0]:\n",
    "            return 1 # Rotate 90 degrees clockwise\n",
    "        elif curr_state[0] < prev_state[0]:\n",
    "            return 3 # Rotate 90 degrees anti-clockwise\n",
    "        elif curr_state[1] < prev_state[1]:\n",
    "            return 2 # Rotate 180 degrees\n",
    "        elif curr_state[1] > prev_state[1]:\n",
    "            return 0 # No roation\n",
    "        else:\n",
    "            return 0 # No rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent class\n",
    "class Agent():\n",
    "    def __init__(self, env, model, target_model):\n",
    "        self.env = env\n",
    "        self.model = model\n",
    "        self.target_model = target_model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        self.gamma = 0.7\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = 0.0003\n",
    "        self.batch_size = 64\n",
    "        self.memory = deque(maxlen=1000)\n",
    "        \n",
    "\n",
    "    def add_to_memory(self, state, goal, action, reward, next_state, done):\n",
    "        self.memory.append((state, goal, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(0, 4)\n",
    "        else:\n",
    "            # Pre-process the input\n",
    "            inputt = np.concatenate((state, self.env.goal))\n",
    "            inputt = tf.convert_to_tensor(inputt)\n",
    "            inputt = tf.expand_dims(inputt, 0)\n",
    "\n",
    "            return np.argmax(self.model.predict(inputt, verbose=0)[0]) # TODO: check the predict output\n",
    "\n",
    "    def predict(self, inputt):\n",
    "\n",
    "        return np.argmax(self.model.predict(inputt, verbose=0)[0])\n",
    "\n",
    "    def replay(self):\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        for state, goal, action, reward, next_state, done in batch:\n",
    "            \n",
    "            target = reward\n",
    "\n",
    "            if not done:\n",
    "                # Pre-process the next state input\n",
    "                input_next = np.concatenate((next_state, goal))\n",
    "                input_next = tf.convert_to_tensor(input_next)\n",
    "                input_next = tf.expand_dims(input_next, 0)\n",
    "\n",
    "                target += self.gamma * np.amax(self.target_model.predict(input_next, verbose=0)[0])\n",
    "\n",
    "            # Pre-process the current state input\n",
    "            inputt = np.concatenate((state, goal))\n",
    "            inputt = tf.convert_to_tensor(inputt)\n",
    "            inputt = tf.expand_dims(inputt, 0)\n",
    "\n",
    "            cur_q_value = self.model.predict(inputt, verbose=0) # Q-value of current state\n",
    "            cur_q_value[0][action] = target # TODO: check the predict output\n",
    "            \n",
    "            self.model.fit(inputt, cur_q_value, epochs=1, verbose=0)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agent\n",
    "\n",
    "try:\n",
    "    model = tf.keras.models.load_model('models/ql_fed_dqn.h5')\n",
    "    print(\"Loaded model from disk\")\n",
    "    agent = Agent(Env(), model=model, target_model=model)\n",
    "except:\n",
    "    print(\"Creating new model\")\n",
    "    agent = Agent(Env(), model=create_model(input_shape=(4,), num_actions=4), target_model=create_model(input_shape=(4,), num_actions=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent\n",
    "success = 0\n",
    "crash = 0\n",
    "no_tests = 1000\n",
    "for i in range(no_tests):\n",
    "    state = agent.env.reset()\n",
    "    goal = agent.env.reset_goal()\n",
    "    print('Goal:', goal)\n",
    "    prev_state = state.copy()\n",
    "    for step in range(50):\n",
    "        print('State:', state)\n",
    "        if agent.env.get_orientation(prev_state, state) == 1:\n",
    "            \n",
    "        inputt = np.concatenate((state, goal))\n",
    "        inputt = tf.convert_to_tensor(inputt)\n",
    "        inputt = tf.expand_dims(inputt, 0)\n",
    "        action = agent.predict(inputt)\n",
    "        next_state, reward, done, terminate = agent.env.step(action)\n",
    "        prev_state, state = state.copy(), next_state.copy()\n",
    "        if done:\n",
    "            print('State:', state)\n",
    "            print('Steps: ', step+1)\n",
    "            if np.array_equal(agent.env.goal, agent.env.pos):\n",
    "                print('Reached the goal!')\n",
    "                success += 1\n",
    "            break\n",
    "        elif terminate:\n",
    "            print('State:', state)\n",
    "            print('Steps: ', step+1)\n",
    "            print('Terminated!')\n",
    "            crash += 1\n",
    "            break\n",
    "\n",
    "print('Success rate:', success/no_tests)\n",
    "print('Crash rate:', crash/no_tests)\n",
    "print('Oscillate rate:', (no_tests - success - crash)/no_tests)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
